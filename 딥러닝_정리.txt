종종 pytorch에서 Dataloader를 사용하다가 에러가 뜬다. 아직 이유가 정확히 뭔지는 모르나 무슨 main문이 필요하다고 뜬다.
임시 방편으로는 Dataloader를 사용할때 인자값으로 num_worker가 있다. 이거를 "0"으로 지정해주면 에러가 뜨진 않는다.
num_worker가 무슨 param인지 알아보자   #TODO LIST




==============================================================================================================================================================

딥러닝 공부하다 보면 weight decay라는 단어가 종종 나온다.
모델을 학습하다 보면 overfitting이 발생할 수 있음. >> 이를 해결 방법 중 데이터를 늘리는 방법이 있다.
하지만 그럴수 없는 상황이라면 weight decay를 이욯해서 overfitting을 예방?할 수 있다.


weight decay : loss func이 작아지는 방양으로만 단순하게 학습을 진행하면, 
                   특정 가중치 값들이 커지면서 결과가 안좋을 수 있다.
                    weight decay는 학습된 모델에 복잡도를 줄위기 위해 학습 중 weight가 너무 커지지 않도록 
                   loss func에 weight가 커질 경우에 대한 패널티 항목을 부여한다. 
                 
이 패널티 항목으로 많이 쓰이는 것이 L1, L2 Regularizstion이다. -> 과적합 방지
L1 : lasso Regrassion, L2 : Ridge Regrassion 				#TODO LIST




==============================================================================================================================================================



batch norm

만들어진 배경  ->  hidden layer에서 next hidden layer로 갈 때 발생하는 ICS를 줄이려고 만들어짐.
		하지만 ICS는 막지 못하고 smoothing 역할만 한다고 함.
		한 마디로 다음 계층으로 가는 connection weights를 정규화해준다고함.
		여기서 정규화는 norm? regularization? 			# TODO LIST






















































































































































































































































