종종 pytorch에서 Dataloader를 사용하다가 에러가 뜬다. 아직 이유가 정확히 뭔지는 모르나 무슨 main문이 필요하다고 뜬다.
임시 방편으로는 Dataloader를 사용할때 인자값으로 num_worker가 있다. 이거를 "0"으로 지정해주면 에러가 뜨진 않는다.
num_worker가 무슨 param인지 알아보자   #TODO LIST




==============================================================================================================================================================

딥러닝 공부하다 보면 weight decay라는 단어가 종종 나온다.
모델을 학습하다 보면 overfitting이 발생할 수 있음. >> 이를 해결 방법 중 데이터를 늘리는 방법이 있다.
하지만 그럴수 없는 상황이라면 weight decay를 이욯해서 overfitting을 예방?할 수 있다.


weight decay : loss func이 작아지는 방양으로만 단순하게 학습을 진행하면, 
                   특정 가중치 값들이 커지면서 결과가 안좋을 수 있다.
                    weight decay는 학습된 모델에 복잡도를 줄위기 위해 학습 중 weight가 너무 커지지 않도록 
                   loss func에 weight가 커질 경우에 대한 패널티 항목을 부여한다. 
                 
이 패널티 항목으로 많이 쓰이는 것이 L1, L2 Regularizstion이다. -> 과적합 방지
L1 : lasso Regrassion, L2 : Ridge Regrassion 				#TODO LIST




==============================================================================================================================================================



batch normalization
		딥러닝 모델을 설계함에 있어서, normalization 방법론을 딥러닝 학습 프로세스에 포함시키는 방법론. 
		이 방법론을 사용하면, 편차가 큰 데이터들을 사용하더라도 local optimization에 빠지는 문제 등을 해결할 수 있으며, outlier에 대한 영향도 줄일 수 있음. 
		따라서, batch norm을 사용하면, 몇몇 상황에서는 regularization 효과를 볼 수 있음.


만들어진 배경  ->  hidden layer에서 next hidden layer로 갈 때 발생하는 ICS를 줄이려고 만들어짐.
		하지만 ICS는 막지 못하고 smoothing 역할만 한다고 함.
		한 마디로 다음 계층으로 가는 connection weights를 정규화해준다고함.
		여기서 정규화는 > normalization		


내부 공변량 변화(Internal Covariate Shift)
배치 정규화를 이해하기 위해서는 내부 공변량 변화(Internal Covariate Shift)를 이해할 필요가 있음.
내부 공변량 변화란 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상을 말함.
이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생.
배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥 러닝 모델의 불안전성이 층마다 입력의 분포가 달라지기 때문이라고 주장함.

공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미합니다.
내부 공변량 변화는 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미합니다.





Regularization ==>  학습된 모델을 일반화 함, 즉 아직 학습하지 않은 다른 데이터들에 대해서도 잘 대응하도록 함. 
		즉 과적합을 방지하도록 함. 이와 같은 형태로 인식할 수 있음. 
		신경망 및 딥러닝 방법론이 나오기 전까지는 Regularization 방법론은 결론을 추론하는 함수에서, 		
		특정 값의 가중치에 페널티를 부여하여 과적합을 방지하는 형태 였음 (Weight decay). 
		최근 신경망 및 딥러닝 방법론이 발달하면서, early stopping, dropout 등이 나오게 되었음. 
		또한 batch normalization기법으로도 일정 수준 regularization 효과를 볼수 있는것으로 확인됨.


그러면 regularization과 normalization의 관계는 어떻게 되는가?
> norm은 말 그대로 데이터를 정규화 한다는 의미. regularization과는 전혀 다른 의미라고 함.




Q. batch normalization 방법론이 ICS를 막지 못하고, 다음 hidden layer로 가는 connection weights를 정규화 해주는 것으로 알 고 있습니다.
그럼 여기서 connection weights{벡터(행렬)}값 들을 정규화 해주는 것이, 수학적(data) 성질을 보전(유지) 하면서 의미 있는 값으로 변경하는 선형사상의 개념과 같다고 생각하면 될까요??

A. 선형사상과 Connection Weight는 다른 개념임. 근본적으로 수학은 연역적 방법을 인공지능은 귀납적 방법을 채택함.



Q. 사영변환의 "R3 상의 벡터를 R2의 평면에 수직으로 변환한다."라는 문장을 차원 축소와 개념을 동일하게 생각해도 될까요?

A. 사영변환이 차원 축소이긴하나 차원 축소가 사영변환이지는 않음
     (a1, a2, a3) --> (a1 + a2, a2-a3)도 차원 축소이고 선형변환인데 사영변환은 아님.

==============================================================================================================================================================

 inductive bias  ==> 학습을 할때의 귀납적 방향성, 예) SVM : 그룹간 마진 최대화, Linear Regression : 데이터의 형태는 선형. 	
		즉, 데이터가 ~~~한 형태를 가질것이다 라는 가정이 inductive bias라고 생각하면됨. 질문에 적어둔, 
		set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered가 induced bias를 잘 표현한 것임.



inductive bias 와 regularization 의 차이점은 무엇?

weight decay 나 early stopping 과 같이 가중치의 크기는 작아야 한다는 prior knowledge 를 넣어주고 있는 경우 
이 또한 inductive bias 인가? 그렇다면 regularization 또한 inductive bias 를 만들어낸다고 봐야하나?

이러한 정의에 의하면, regularization이라는 의미가 inductive bias를 만들어 낸다는 표현은 부적절한 표현으로 보임.
weight decay, early stopping이 가중치의 크기가 작아야한다라는 가정에 의한것은 아니고, 해당 regularization 방법론을 적용하다보면 가중치가 작아지는것임.
만약, 가중치를 제한한다는 의미를 inductive bias로 가정하고 모델의 학습 프로세스를 설계한다면, 새로운 시도가 될것으로 생각됨s


=============================================================================================================================================================================








Drop out : 





drop out은 모델을 regularization 해준다.
drop out의 주 목적은 과적합을 방지해준다.































































































































































































































