종종 pytorch에서 Dataloader를 사용하다가 에러가 뜬다. 아직 이유가 정확히 뭔지는 모르나 무슨 main문이 필요하다고 뜬다.
임시 방편으로는 Dataloader를 사용할때 인자값으로 num_worker가 있다. 이거를 "0"으로 지정해주면 에러가 뜨진 않는다.
num_worker가 무슨 param인지 알아보자   #TODO LIST




==============================================================================================================================================================

딥러닝 공부하다 보면 weight decay라는 단어가 종종 나온다.
모델을 학습하다 보면 overfitting이 발생할 수 있음. >> 이를 해결 방법 중 데이터를 늘리는 방법이 있다.
하지만 그럴수 없는 상황이라면 weight decay를 이욯해서 overfitting을 예방?할 수 있다.


weight decay : loss func이 작아지는 방양으로만 단순하게 학습을 진행하면, 
                   특정 가중치 값들이 커지면서 결과가 안좋을 수 있다.
                    weight decay는 학습된 모델에 복잡도를 줄위기 위해 학습 중 weight가 너무 커지지 않도록 
                   loss func에 weight가 커질 경우에 대한 패널티 항목을 부여한다. 
                 
이 패널티 항목으로 많이 쓰이는 것이 L1, L2 Regularizstion이다. -> 과적합 방지
L1 : lasso Regrassion, L2 : Ridge Regrassion 				#TODO LIST




==============================================================================================================================================================



batch norm

만들어진 배경  ->  hidden layer에서 next hidden layer로 갈 때 발생하는 ICS를 줄이려고 만들어짐.
		하지만 ICS는 막지 못하고 smoothing 역할만 한다고 함.
		한 마디로 다음 계층으로 가는 connection weights를 정규화해준다고함.
		여기서 정규화는 > normalization		




Regularization ==>  학습된 모델을 일반화 함, 즉 아직 학습하지 않은 다른 데이터들에 대해서도 잘 대응하도록 함. 
		즉 과적합을 방지하도록 함. 이와 같은 형태로 인식할 수 있음. 
		신경망 및 딥러닝 방법론이 나오기 전까지는 Regularization 방법론은 결론을 추론하는 함수에서, 		
		특정 값의 가중치에 페널티를 부여하여 과적합을 방지하는 형태 였음 (Weight decay). 
		최근 신경망 및 딥러닝 방법론이 발달하면서, early stopping, dropout 등이 나오게 되었음. 
		또한 batch normalization기법으로도 일정 수준 regularization 효과를 볼수 있는것으로 확인됨.

그러면 regularization과 normalization의 관계는 어떻게 되는가?
> norm은 말 그대로 데이터를 정규화 한다는 의미. regularization과는 전혀 다른 의미라고 함.


==============================================================================================================================================================

 inductive bias  ==> 학습을 할때의 귀납적 방향성, 예) SVM : 그룹간 마진 최대화, Linear Regression : 데이터의 형태는 선형. 	
		즉, 데이터가 ~~~한 형태를 가질것이다 라는 가정이 inductive bias라고 생각하면됨. 질문에 적어둔, 
		set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered가 induced bias를 잘 표현한 것임.




















































































































































































































































